{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import pywt\n",
    "from scipy import stats\n",
    "from multiprocessing import Pool, set_start_method\n",
    "import os\n",
    "set_start_method('fork')\n",
    "\n",
    "# 1. 数据预处理部分\n",
    "def convert_pe_to_image(file_path, output_path, width=384):\n",
    "    \"\"\"将PE文件转换为灰度图像\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            content = f.read()\n",
    "            \n",
    "        # 将字节转换为uint8数组\n",
    "        byte_array = np.frombuffer(content, dtype=np.uint8)\n",
    "        \n",
    "        # 计算需要的行数\n",
    "        height = len(byte_array) // width + (1 if len(byte_array) % width else 0)\n",
    "        \n",
    "        # 填充数组到完整的矩形\n",
    "        padded_size = height * width\n",
    "        if len(byte_array) < padded_size:\n",
    "            byte_array = np.pad(byte_array, (0, padded_size - len(byte_array)))\n",
    "            \n",
    "        # 重塑为2D数组\n",
    "        image_array = byte_array.reshape((height, width))\n",
    "        \n",
    "        # 创建并保存图像\n",
    "        image = Image.fromarray(image_array)\n",
    "        image.save(output_path)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def process_single_file(args):\n",
    "    \"\"\"处理单个PE文件的辅助函数\"\"\"\n",
    "    input_path, output_path = args\n",
    "    success = convert_pe_to_image(input_path, output_path)\n",
    "    return success\n",
    "\n",
    "def process_directory(input_dir, output_dir, num_processes=None):\n",
    "    \"\"\"使用多进程并行处理整个目录的PE文件\"\"\"\n",
    "    # 创建输出目录\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 准备任务列表\n",
    "    tasks = []\n",
    "    for filename in os.listdir(input_dir):\n",
    "        input_path = os.path.join(input_dir, filename)\n",
    "        output_path = os.path.join(output_dir, f\"{filename}.png\")\n",
    "        tasks.append((input_path, output_path))\n",
    "    \n",
    "    total_count = len(tasks)\n",
    "    \n",
    "    # 使用进程池并行处理\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        results = pool.map(process_single_file, tasks)\n",
    "    \n",
    "    # 统计成功数量\n",
    "    success_count = sum(1 for result in results if result)\n",
    "    \n",
    "    print(f\"Successfully processed {success_count}/{total_count} files\")\n",
    "\n",
    "\n",
    "# 2. 模型定义\n",
    "class MalwareDetectionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MalwareDetectionCNN, self).__init__()\n",
    "        # 增加卷积层数量\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)  # 新增卷积层\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # 使用自适应平均池化将特征图转换为固定大小\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((6, 8))\n",
    "        \n",
    "        # 根据自适应池化后的固定输出大小计算全连接层输入\n",
    "        self.fc1 = nn.Linear(256 * 6 * 8, 1024)  # 增加神经元数量\n",
    "        self.fc2 = nn.Linear(1024, 512)  # 增加神经元数量\n",
    "        self.fc3 = nn.Linear(512, 2)  # 新增全连接层\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 卷积层\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))  # 新增卷积层的前向传播\n",
    "        # 自适应池化到固定大小\n",
    "        x = self.adaptive_pool(x)\n",
    "        \n",
    "        # 展平并通过全连接层\n",
    "        x = x.view(-1, 256 * 6 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))  # 新增全连接层的前向传播\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def get_features(self, x):\n",
    "        \"\"\"提取中间特征的方法\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # 存储每个卷积层的输出\n",
    "        x1 = self.pool(F.relu(self.conv1(x)))\n",
    "        features.append(x1)\n",
    "        \n",
    "        x2 = self.pool(F.relu(self.conv2(x1)))\n",
    "        features.append(x2)\n",
    "        \n",
    "        x3 = self.pool(F.relu(self.conv3(x2)))\n",
    "        features.append(x3)\n",
    "        \n",
    "        # 自适应池化后的特征\n",
    "        x4 = self.adaptive_pool(x3)\n",
    "        features.append(x4)\n",
    "        \n",
    "        return features\n",
    "# 3. 数据集类\n",
    "class MalwareDataset(Dataset):\n",
    "    def __init__(self, benign_dir, malware_dir, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        \n",
    "        # 加载良性样本\n",
    "        for img_name in os.listdir(benign_dir):\n",
    "            if img_name.endswith('.png'):\n",
    "                self.data.append((os.path.join(benign_dir, img_name), 0))\n",
    "                \n",
    "        # 加载恶意样本\n",
    "        for img_name in os.listdir(malware_dir):\n",
    "            if img_name.endswith('.png'):\n",
    "                self.data.append((os.path.join(malware_dir, img_name), 1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "        image = Image.open(img_path).convert('L')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "# 4. 训练函数\n",
    "def train_model(benign_dir, malware_dir, model_save_path, epochs=10, batch_size=32):\n",
    "    # 设置设备\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # 数据转换\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((384, 512)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    # 创建数据集\n",
    "    dataset = MalwareDataset(benign_dir, malware_dir, transform=transform)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # 创建模型\n",
    "    model = MalwareDetectionCNN().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        # 验证\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "        print(f'Epoch {epoch + 1}, Loss: {running_loss/len(train_loader):.3f}, '\n",
    "              f'Accuracy: {100 * correct / total:.2f}%')\n",
    "    \n",
    "    # 保存模型\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "def extract_wavelet_features(image_path, wavelet='db1', level=3):\n",
    "    \"\"\"\n",
    "    使用小波变换从图像中提取特征\n",
    "    \"\"\"\n",
    "    # 读取图像\n",
    "    img = Image.open(image_path).convert('L')\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    # 执行多级二维离散小波变换\n",
    "    coeffs = pywt.wavedec2(img_array, wavelet, level=level)\n",
    "    \n",
    "    # 提取特征\n",
    "    features = []\n",
    "    \n",
    "    # 处理近似系数\n",
    "    features.extend([\n",
    "        np.mean(coeffs[0]),\n",
    "        np.std(coeffs[0]),\n",
    "        stats.skew(coeffs[0].ravel()),\n",
    "        stats.kurtosis(coeffs[0].ravel())\n",
    "    ])\n",
    "    \n",
    "    # 处理细节系数\n",
    "    for detail_coeffs in coeffs[1:]:\n",
    "        for detail in detail_coeffs:\n",
    "            features.extend([\n",
    "                np.mean(detail),\n",
    "                np.std(detail),\n",
    "                stats.skew(detail.ravel()),\n",
    "                stats.kurtosis(detail.ravel())\n",
    "            ])\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# 基于小波特征的数据集\n",
    "class WaveletMalwareDataset(Dataset):\n",
    "    def __init__(self, benign_dir, malware_dir):\n",
    "        self.data = []\n",
    "        \n",
    "        # 加载良性样本\n",
    "        for img_name in os.listdir(benign_dir):\n",
    "            if img_name.endswith('.png'):\n",
    "                features = extract_wavelet_features(os.path.join(benign_dir, img_name))\n",
    "                self.data.append((features, 0))\n",
    "                \n",
    "        # 加载恶意样本\n",
    "        for img_name in os.listdir(malware_dir):\n",
    "            if img_name.endswith('.png'):\n",
    "                features = extract_wavelet_features(os.path.join(malware_dir, img_name))\n",
    "                self.data.append((features, 1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features, label = self.data[idx]\n",
    "        return torch.FloatTensor(features), label\n",
    "\n",
    "# 基于小波特征的神经网络模型\n",
    "class WaveletMalwareDetector(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(WaveletMalwareDetector, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# 训练小波特征模型\n",
    "def train_wavelet_model(benign_dir, malware_dir, model_save_path, epochs=20, batch_size=32):\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    \n",
    "    # 加载数据集\n",
    "    dataset = torch.load(\"wavelet_dataset.pth\", weights_only=False)\n",
    "    \n",
    "    # 确定输入特征维度\n",
    "    input_size = dataset[0][0].shape[0]\n",
    "    \n",
    "    # 分割训练集和测试集\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # 创建模型\n",
    "    model = WaveletMalwareDetector(input_size).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (features, labels) in enumerate(train_loader):\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        # 验证\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for features, labels in test_loader:\n",
    "                features, labels = features.to(device), labels.to(device)\n",
    "                outputs = model(features)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "        print(f'Epoch {epoch + 1}, Loss: {running_loss/len(train_loader):.3f}, '\n",
    "              f'Accuracy: {100 * correct / total:.2f}%')\n",
    "    \n",
    "    # 保存模型\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Wavelet model saved to {model_save_path}\")\n",
    "    return input_size\n",
    "def scan_file(cnn_model_path, wavelet_model_path, wavelet_input_size, file_path, device=None):\n",
    "    \"\"\"整合CNN和小波特征模型的扫描函数\"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    \n",
    "    # 创建临时图像\n",
    "    temp_img_path = \"temp_scan.png\"\n",
    "    if not convert_pe_to_image(file_path, temp_img_path):\n",
    "        return None, None\n",
    "    \n",
    "    # 加载CNN模型\n",
    "    cnn_model = MalwareDetectionCNN().to(device)\n",
    "    cnn_model.load_state_dict(torch.load(cnn_model_path))\n",
    "    cnn_model.eval()\n",
    "    \n",
    "    # 加载小波特征模型\n",
    "    wavelet_model = WaveletMalwareDetector(wavelet_input_size).to(device)\n",
    "    wavelet_model.load_state_dict(torch.load(wavelet_model_path))\n",
    "    wavelet_model.eval()\n",
    "    \n",
    "    # CNN预测\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((384, 512)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    image = Image.open(temp_img_path).convert('L')\n",
    "    cnn_input = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # 小波特征预测\n",
    "    wavelet_features = extract_wavelet_features(temp_img_path)\n",
    "    wavelet_input = torch.FloatTensor(wavelet_features).unsqueeze(0).to(device)\n",
    "    \n",
    "    # 获取两个模型的预测结果\n",
    "    with torch.no_grad():\n",
    "        cnn_outputs = cnn_model(cnn_input)\n",
    "        wavelet_outputs = wavelet_model(wavelet_input)\n",
    "        \n",
    "        cnn_probs = F.softmax(cnn_outputs, dim=1)\n",
    "        wavelet_probs = F.softmax(wavelet_outputs, dim=1)\n",
    "        \n",
    "        # 计算平均恶意分数\n",
    "        cnn_score = cnn_probs[0][1].item()\n",
    "        wavelet_score = wavelet_probs[0][1].item()\n",
    "        average_score = (cnn_score + wavelet_score) / 2\n",
    "        \n",
    "        # 根据平均分数决定最终预测\n",
    "        final_prediction = 1 if average_score > 0.6 else 0\n",
    "    \n",
    "    # 清理临时文件\n",
    "    os.remove(temp_img_path)\n",
    "    \n",
    "    return final_prediction, average_score\n",
    "\n",
    "def scan_directory(cnn_model_path, wavelet_model_path, wavelet_input_size, scan_dir):\n",
    "    \"\"\"使用两个模型扫描目录\"\"\"\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    total_files = 0\n",
    "    detected_malware = 0\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for filename in os.listdir(scan_dir):\n",
    "        file_path = os.path.join(scan_dir, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            total_files += 1\n",
    "            prediction, malware_score = scan_file(\n",
    "                cnn_model_path, \n",
    "                wavelet_model_path, \n",
    "                wavelet_input_size,\n",
    "                file_path, \n",
    "                device\n",
    "            )\n",
    "            \n",
    "            if prediction is not None:\n",
    "                results.append({\n",
    "                    'file': filename,\n",
    "                    'is_malware': bool(prediction),\n",
    "                    'malware_score': malware_score\n",
    "                })\n",
    "                \n",
    "                if prediction == 1:\n",
    "                    detected_malware += 1\n",
    "    \n",
    "    detection_rate = (detected_malware / total_files * 100) if total_files > 0 else 0\n",
    "    return results, detection_rate\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 974/974 files\n",
      "Successfully processed 1102/1102 files\n"
     ]
    }
   ],
   "source": [
    "# 1. 预处理数据\n",
    "process_directory(\"black_files\", \"black_images\", 8)\n",
    "process_directory(\"white_files\", \"white_images\", 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Nozomi/Download/Malware/training/.venv/lib/python3.9/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (104810880 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/Volumes/Nozomi/Download/Malware/training/.venv/lib/python3.9/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (93323520 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.663, Accuracy: 73.60%\n",
      "Epoch 2, Loss: 0.557, Accuracy: 76.40%\n",
      "Epoch 3, Loss: 0.491, Accuracy: 79.44%\n",
      "Epoch 4, Loss: 0.421, Accuracy: 82.01%\n",
      "Epoch 5, Loss: 0.373, Accuracy: 84.11%\n",
      "Epoch 6, Loss: 0.367, Accuracy: 85.75%\n",
      "Epoch 7, Loss: 0.338, Accuracy: 86.45%\n",
      "Epoch 8, Loss: 0.299, Accuracy: 85.28%\n",
      "Epoch 9, Loss: 0.283, Accuracy: 86.92%\n",
      "Epoch 10, Loss: 0.267, Accuracy: 87.38%\n",
      "Model saved to cnn_detector.pth\n"
     ]
    }
   ],
   "source": [
    "train_model(\"white_images\", \"black_images\", \"cnn_detector.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Nozomi/Download/Malware/training/.venv/lib/python3.9/site-packages/pywt/_multilevel.py:43: UserWarning: Level value of 3 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 创建数据集\n",
    "dataset = WaveletMalwareDataset(\"white_images\", \"black_images\")\n",
    "torch.save(dataset, \"wavelet_dataset.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.178, Accuracy: 65.89%\n",
      "Epoch 2, Loss: 0.726, Accuracy: 56.31%\n",
      "Epoch 3, Loss: 0.715, Accuracy: 66.12%\n",
      "Epoch 4, Loss: 0.681, Accuracy: 63.32%\n",
      "Epoch 5, Loss: 0.649, Accuracy: 76.17%\n",
      "Epoch 6, Loss: 0.596, Accuracy: 69.16%\n",
      "Epoch 7, Loss: 0.562, Accuracy: 80.37%\n",
      "Epoch 8, Loss: 0.522, Accuracy: 82.01%\n",
      "Epoch 9, Loss: 0.508, Accuracy: 83.18%\n",
      "Epoch 10, Loss: 0.480, Accuracy: 83.88%\n",
      "Epoch 11, Loss: 0.460, Accuracy: 83.64%\n",
      "Epoch 12, Loss: 0.462, Accuracy: 84.58%\n",
      "Epoch 13, Loss: 0.421, Accuracy: 83.41%\n",
      "Epoch 14, Loss: 0.439, Accuracy: 85.28%\n",
      "Epoch 15, Loss: 0.418, Accuracy: 85.98%\n",
      "Epoch 16, Loss: 0.412, Accuracy: 84.35%\n",
      "Epoch 17, Loss: 0.405, Accuracy: 87.15%\n",
      "Epoch 18, Loss: 0.407, Accuracy: 85.05%\n",
      "Epoch 19, Loss: 0.412, Accuracy: 85.28%\n",
      "Epoch 20, Loss: 0.390, Accuracy: 86.45%\n",
      "Wavelet model saved to wavelet_detector.pth\n"
     ]
    }
   ],
   "source": [
    "wavelet_input_size = train_wavelet_model(\"white_images\", \"black_images\", \"wavelet_detector.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "print(wavelet_input_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
