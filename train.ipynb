{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pefile\n",
    "import pywt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "\n",
    "def calculate_entropy(data):\n",
    "    if not data:\n",
    "        return 0.0\n",
    "    counts = Counter(data)\n",
    "    entropy = 0.0\n",
    "    total = len(data)\n",
    "    for count in counts.values():\n",
    "        p = count / total\n",
    "        entropy -= p * math.log2(p)\n",
    "    return entropy\n",
    "\n",
    "def get_import_features(pe, max_dim=128):\n",
    "    features = [0] * max_dim\n",
    "    try:\n",
    "        if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT'):\n",
    "            imports = []\n",
    "            for entry in pe.DIRECTORY_ENTRY_IMPORT:\n",
    "                for imp in entry.imports:\n",
    "                    if imp.name:\n",
    "                        imports.append(imp.name.decode('utf-8', 'ignore'))\n",
    "            for i, func in enumerate(imports[:max_dim]):\n",
    "                features[i] = hash(func) % 1000  # Hash simplification\n",
    "    except:\n",
    "        pass\n",
    "    return features\n",
    "\n",
    "def get_section_features(pe, max_sections=8):\n",
    "    sections = []\n",
    "    try:\n",
    "        for section in pe.sections[:max_sections]:\n",
    "            sections.extend([\n",
    "                section.get_entropy(),\n",
    "                section.SizeOfRawData,\n",
    "                section.Characteristics\n",
    "            ])\n",
    "    except:\n",
    "        pass\n",
    "    pad_length = max_sections*3 - len(sections)\n",
    "    return sections + [0]*pad_length\n",
    "\n",
    "def get_image_features(file_path, img_size=64):\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = f.read()\n",
    "        \n",
    "        arr = np.frombuffer(data, dtype=np.uint8)\n",
    "        \n",
    "        actual_size = min(img_size, int(np.ceil(np.sqrt(len(arr)))))\n",
    "        actual_size = actual_size - (actual_size % 2)  # Ensure even size for wavelet\n",
    "        \n",
    "        # Reshape\n",
    "        arr = arr[:actual_size*actual_size]\n",
    "        arr = np.pad(arr, (0, actual_size*actual_size - len(arr)))\n",
    "        img = arr.reshape(actual_size, actual_size).astype(float)\n",
    "        \n",
    "        img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
    "        \n",
    "        coeffs = pywt.wavedec2(img, 'haar', level=3)\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        for coef in coeffs:\n",
    "            if isinstance(coef, tuple):\n",
    "                for detail_coef in coef:\n",
    "                    features.extend([\n",
    "                        np.mean(np.abs(detail_coef)),  # Mean energy\n",
    "                        np.std(detail_coef),           # Standard deviation\n",
    "                        np.percentile(detail_coef, 90),# 90th percentile\n",
    "                        np.sum(detail_coef < 0),       # Number of negative coefficients\n",
    "                        entropy_measure(detail_coef)    # Entropy of coefficients\n",
    "                    ])\n",
    "            else:\n",
    "                features.extend([\n",
    "                    np.mean(np.abs(coef)),            # Mean energy\n",
    "                    np.std(coef),                     # Standard deviation\n",
    "                    np.percentile(coef, 90),          # 90th percentile\n",
    "                    np.sum(coef < 0),                 # Number of negative coefficients\n",
    "                    entropy_measure(coef)             # Entropy of coefficients\n",
    "                ])\n",
    "        features.extend(coeffs[0].flatten().tolist()) # Approximation coefficients\n",
    "        expected_length = 1024\n",
    "        if len(features) < expected_length:\n",
    "            features.extend([0.0] * (expected_length - len(features)))\n",
    "        \n",
    "        return features[:expected_length]\n",
    "    \n",
    "    except Exception as e:\n",
    "        return [0.0] * ((1 + 3*2) * 5)\n",
    "\n",
    "def entropy_measure(coeffs):\n",
    "    coeffs = np.abs(coeffs)\n",
    "    coeffs = coeffs / (np.sum(coeffs) + 1e-8)\n",
    "    entropy = -np.sum(coeffs * np.log2(coeffs + 1e-8))\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def extract_file_features(file_path):\n",
    "    try:\n",
    "        pe = pefile.PE(file_path)\n",
    "    except:\n",
    "        pe = None\n",
    "    \n",
    "    features = []\n",
    "    # Entropy\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = f.read()\n",
    "    features.append(calculate_entropy(data))\n",
    "    \n",
    "    # Import features\n",
    "    features += get_import_features(pe) if pe else [0]*128\n",
    "    \n",
    "    # Section features\n",
    "    features += get_section_features(pe) if pe else [0]*24\n",
    "    \n",
    "    # Wavelet features\n",
    "    features += get_image_features(file_path)\n",
    "    \n",
    "    return np.array(features, dtype=np.float32)\n",
    "\n",
    "def process_file(args):\n",
    "    file_path, label = args\n",
    "    features = extract_file_features(file_path)\n",
    "    return features, label\n",
    "\n",
    "def prepare_dataset(black_dir='black_files', white_dir='white_files', save_path='dataset', num_processes=6):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # default: use available CPU cores\n",
    "    if num_processes is None:\n",
    "        num_processes = mp.cpu_count()\n",
    "    \n",
    "    file_list = []\n",
    "    for label, path in enumerate([white_dir, black_dir]):\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"Warning: Directory {path} does not exist.\")\n",
    "            continue\n",
    "        for root, _, files in os.walk(path):\n",
    "            for f in files:\n",
    "                file_path = os.path.join(root, f)\n",
    "                file_list.append((file_path, 1 if label else 0))\n",
    "    \n",
    "    print(f\"Processing {len(file_list)} files using {num_processes} processes...\")\n",
    "    \n",
    "    # Process files in parallel\n",
    "    X, y = [], []\n",
    "    with mp.Pool(processes=num_processes) as pool:\n",
    "        for features, label in tqdm(pool.imap(process_file, file_list), total=len(file_list)):\n",
    "            X.append(features)\n",
    "            y.append(label)\n",
    "    \n",
    "    X = np.stack(X) if X else np.array([])\n",
    "    y = np.array(y)\n",
    "    \n",
    "    np.save(os.path.join(save_path, 'features.npy'), X)\n",
    "    np.save(os.path.join(save_path, 'labels.npy'), y)\n",
    "    print(f\"Dataset saved with {len(X)} samples\")\n",
    "\n",
    "\n",
    "class MalwareClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class MalwareDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.X = torch.FloatTensor(features)\n",
    "        self.y = torch.FloatTensor(labels)\n",
    "        self.mean = self.X.mean(0)\n",
    "        self.std = self.X.std(0)\n",
    "        self.std[self.std == 0] = 1.0\n",
    "        self.X = (self.X - self.mean) / self.std\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def train_model(features_path, labels_path, model_save='model.pt'):\n",
    "    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "    \n",
    "    X = np.load(features_path)\n",
    "    y = np.load(labels_path)\n",
    "    dataset = MalwareDataset(X, y)\n",
    "    \n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    train_set, val_set = torch.utils.data.random_split(\n",
    "        dataset, [train_size, len(dataset)-train_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=128)\n",
    "\n",
    "    model = MalwareClassifier(X.shape[1]).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    best_acc = 0\n",
    "    for epoch in range(20):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device).unsqueeze(1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val = X_val.to(device)\n",
    "                y_val = y_val.to(device).unsqueeze(1)\n",
    "                outputs = model(X_val)\n",
    "                predicted = (outputs >= 0.5).float()\n",
    "                correct += (predicted == y_val).sum().item()\n",
    "                total += y_val.size(0)\n",
    "        acc = correct / total\n",
    "        \n",
    "        print(f'Epoch {epoch+1:2d} | Loss: {total_loss/len(train_loader):.4f} | Acc: {acc:.4f}')\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save({\n",
    "                'model': model.state_dict(),\n",
    "                'mean': dataset.mean,\n",
    "                'std': dataset.std\n",
    "            }, model_save)\n",
    "    \n",
    "    print(f'Training complete. Best accuracy: {best_acc:.4f}')\n",
    "\n",
    "def scan(model_path='model.pt', scan_dir='scan_files'):\n",
    "    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model = MalwareClassifier(len(checkpoint['mean'])).to(device)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    model.eval()\n",
    "    \n",
    "    # move to CPU\n",
    "    mean_np = checkpoint['mean'].cpu().numpy()\n",
    "    std_np = checkpoint['std'].cpu().numpy()\n",
    "    \n",
    "    total = 0\n",
    "    detected = 0\n",
    "    for root, _, files in os.walk(scan_dir):\n",
    "        for f in files:\n",
    "            file_path = os.path.join(root, f)\n",
    "            features = extract_file_features(file_path)\n",
    "            \n",
    "            features = (features - mean_np) / std_np\n",
    "            features_tensor = torch.FloatTensor(features).unsqueeze(0).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                prob = model(features_tensor).item()\n",
    "            \n",
    "            result = 'Malicious' if prob >= 0.5 else 'Benign'\n",
    "            print(f'{f}: {result} ({prob:.4f})')\n",
    "            \n",
    "            total += 1\n",
    "            if prob >= 0.5:\n",
    "                detected += 1\n",
    "    \n",
    "    if total > 0:\n",
    "        print(f'\\nDetection Rate: {detected/total:.2%} ({detected}/{total})')\n",
    "    else:\n",
    "        print('No files found.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2076 files using 6 processes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2076/2076 [01:17<00:00, 26.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved with 2076 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    mp.set_start_method('fork')\n",
    "    prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Loss: 0.6158 | Acc: 0.6947\n",
      "Epoch  2 | Loss: 0.5046 | Acc: 0.7716\n",
      "Epoch  3 | Loss: 0.4673 | Acc: 0.8053\n",
      "Epoch  4 | Loss: 0.4248 | Acc: 0.8173\n",
      "Epoch  5 | Loss: 0.3959 | Acc: 0.8317\n",
      "Epoch  6 | Loss: 0.3773 | Acc: 0.8317\n",
      "Epoch  7 | Loss: 0.3500 | Acc: 0.8438\n",
      "Epoch  8 | Loss: 0.3397 | Acc: 0.8438\n",
      "Epoch  9 | Loss: 0.3186 | Acc: 0.8582\n",
      "Epoch 10 | Loss: 0.3092 | Acc: 0.8462\n",
      "Epoch 11 | Loss: 0.2928 | Acc: 0.8606\n",
      "Epoch 12 | Loss: 0.2845 | Acc: 0.8438\n",
      "Epoch 13 | Loss: 0.2731 | Acc: 0.8654\n",
      "Epoch 14 | Loss: 0.2648 | Acc: 0.8582\n",
      "Epoch 15 | Loss: 0.2537 | Acc: 0.8822\n",
      "Epoch 16 | Loss: 0.2503 | Acc: 0.8798\n",
      "Epoch 17 | Loss: 0.2393 | Acc: 0.8582\n",
      "Epoch 18 | Loss: 0.2334 | Acc: 0.8702\n",
      "Epoch 19 | Loss: 0.2262 | Acc: 0.8702\n",
      "Epoch 20 | Loss: 0.2216 | Acc: 0.8702\n",
      "Training complete. Best accuracy: 0.8822\n"
     ]
    }
   ],
   "source": [
    "train_model('dataset/features.npy', 'dataset/labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.exe: Malicious (0.9349)\n",
      "1.exe: Malicious (0.9349)\n",
      "\n",
      "Detection Rate: 100.00% (2/2)\n"
     ]
    }
   ],
   "source": [
    "scan()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
