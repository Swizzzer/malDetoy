{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pefile\n",
    "import pywt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def calculate_entropy(data):\n",
    "    if not data:\n",
    "        return 0.0\n",
    "    counts = Counter(data)\n",
    "    entropy = 0.0\n",
    "    total = len(data)\n",
    "    for count in counts.values():\n",
    "        p = count / total\n",
    "        entropy -= p * math.log2(p)\n",
    "    return entropy\n",
    "def get_export_features(pe, max_dim=128):\n",
    "    features = [0] * max_dim\n",
    "    try:\n",
    "        if hasattr(pe, 'DIRECTORY_ENTRY_EXPORT'):\n",
    "            exports = [entry.name.decode('utf-8', 'ignore') for entry in pe.DIRECTORY_ENTRY_EXPORT.symbols]\n",
    "            for i, func in enumerate(exports[:max_dim]):\n",
    "                features[i] = calculate_entropy(func.encode('utf-8'))\n",
    "    except:\n",
    "        pass\n",
    "    return features\n",
    "def get_import_features(pe, max_dim=128):\n",
    "    features = [0] * max_dim\n",
    "    try:\n",
    "        if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT'):\n",
    "            imports = []\n",
    "            for entry in pe.DIRECTORY_ENTRY_IMPORT:\n",
    "                for imp in entry.imports:\n",
    "                    if imp.name:\n",
    "                        imports.append(imp.name.decode('utf-8', 'ignore'))\n",
    "            for i, func in enumerate(imports[:max_dim]):\n",
    "                features[i] = hash(func) % 1000  # Hash simplification\n",
    "    except:\n",
    "        pass\n",
    "    return features\n",
    "\n",
    "def get_section_features(pe, max_sections=8):\n",
    "    sections = []\n",
    "    try:\n",
    "        for section in pe.sections[:max_sections]:\n",
    "            sections.extend([\n",
    "                section.get_entropy(),\n",
    "                section.SizeOfRawData,\n",
    "                section.Characteristics              \n",
    "            ])\n",
    "    except:\n",
    "        print(\"Error in get_section_features\")\n",
    "        pass\n",
    "    pad_length = max_sections*3 - len(sections)\n",
    "    return sections + [0]*pad_length\n",
    "\n",
    "def get_image_features(file_path):\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = f.read()\n",
    "        \n",
    "        arr = np.frombuffer(data, dtype=np.uint8)\n",
    "        \n",
    "        actual_size = int(np.ceil(np.sqrt(len(arr))))\n",
    "        actual_size = actual_size - (actual_size % 2)  # Ensure even size for wavelet\n",
    "        \n",
    "        # Reshape\n",
    "        arr = arr[:actual_size*actual_size]\n",
    "        arr = np.pad(arr, (0, actual_size*actual_size - len(arr)))\n",
    "        img = arr.reshape(actual_size, actual_size).astype(float)\n",
    "        \n",
    "        img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
    "        \n",
    "        coeffs = pywt.wavedec2(img, 'haar', level=3)\n",
    "        features = []\n",
    "        \n",
    "        for coef in coeffs:\n",
    "        \n",
    "            if isinstance(coef, tuple):\n",
    "                for detail_coef in coef:\n",
    "                    features.extend([\n",
    "                        np.mean(np.abs(detail_coef)),  # Mean energy\n",
    "                        np.std(detail_coef),           # Standard deviation\n",
    "                        np.percentile(detail_coef, 90),# 90th percentile\n",
    "                        np.sum(detail_coef < 0),       # Number of negative coefficients\n",
    "                        entropy_measure(detail_coef)    # Entropy of coefficients\n",
    "                    ])\n",
    "            else:\n",
    "                features.extend([\n",
    "                    np.mean(np.abs(coef)),            # Mean energy\n",
    "                    np.std(coef),                     # Standard deviation\n",
    "                    np.percentile(coef, 45),          # 90th percentile\n",
    "                    np.sum(coef < 0),                 # Number of negative coefficients\n",
    "                    entropy_measure(coef)             # Entropy of coefficients\n",
    "                ])\n",
    "        for coef in coeffs:\n",
    "            flattened = np.array(coef).flatten()\n",
    "            features.extend(flattened.tolist())\n",
    "        expected_length = 384\n",
    "        if len(features) < expected_length:\n",
    "            features.extend([0.0] * (expected_length - len(features)))\n",
    "        \n",
    "        return features[:expected_length]\n",
    "    \n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        return [0.0] * 384\n",
    "\n",
    "def entropy_measure(coeffs):\n",
    "    coeffs = np.abs(coeffs)\n",
    "    coeffs = coeffs / (np.sum(coeffs) + 1e-8)\n",
    "    entropy = -np.sum(coeffs * np.log2(coeffs + 1e-8))\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def extract_file_features(file_path):\n",
    "    try:\n",
    "        pe = pefile.PE(file_path)\n",
    "    except:\n",
    "        pe = None\n",
    "    \n",
    "    features = []\n",
    "    # Entropy\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = f.read()\n",
    "    features.append(calculate_entropy(data))\n",
    "    \n",
    "    # Import features\n",
    "    features += get_import_features(pe) if pe else [0]*128\n",
    "    # Export features\n",
    "    features += get_export_features(pe) if pe else [0]*128\n",
    "    # Section features\n",
    "    features += get_section_features(pe) if pe else [0]*24\n",
    "    \n",
    "    # Wavelet features\n",
    "    features += get_image_features(file_path)\n",
    "    \n",
    "    return np.array(features, dtype=np.float32)\n",
    "\n",
    "def process_file(args):\n",
    "    file_path, label = args\n",
    "    features = extract_file_features(file_path)\n",
    "    return features, label\n",
    "\n",
    "def prepare_dataset(black_dir='black_files', white_dir='white_files', save_path='dataset', num_processes=6):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # default: use available CPU cores\n",
    "    if num_processes is None:\n",
    "        num_processes = mp.cpu_count()\n",
    "    \n",
    "    file_list = []\n",
    "    for label, path in enumerate([white_dir, black_dir]):\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"Warning: Directory {path} does not exist.\")\n",
    "            continue\n",
    "        for root, _, files in os.walk(path):\n",
    "            for f in files:\n",
    "                file_path = os.path.join(root, f)\n",
    "                file_list.append((file_path, 1 if label else 0))\n",
    "    \n",
    "    print(f\"Processing {len(file_list)} files using {num_processes} processes...\")\n",
    "    \n",
    "    # Process files in parallel\n",
    "    X, y = [], []\n",
    "    with mp.Pool(processes=num_processes) as pool:\n",
    "        for features, label in tqdm(pool.imap(process_file, file_list), total=len(file_list)):\n",
    "            X.append(features)\n",
    "            y.append(label)\n",
    "    \n",
    "    X = np.stack(X) if X else np.array([])\n",
    "    y = np.array(y)\n",
    "    \n",
    "    np.save(os.path.join(save_path, 'features.npy'), X)\n",
    "    np.save(os.path.join(save_path, 'labels.npy'), y)\n",
    "    print(f\"Dataset saved with {len(X)} samples\")\n",
    "\n",
    "\n",
    "\n",
    "class MalwareClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(256 * (input_size // 8), 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "class MalwareDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.X = torch.FloatTensor(features)\n",
    "        self.y = torch.FloatTensor(labels)\n",
    "        self.mean = self.X.mean(0)\n",
    "        self.std = self.X.std(0)\n",
    "        self.std[self.std == 0] = 1.0\n",
    "        self.X = (self.X - self.mean) / self.std\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def train_model(features_path, labels_path, model_save='model.pt'):\n",
    "    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "    \n",
    "    X = np.load(features_path)\n",
    "    y = np.load(labels_path)\n",
    "    dataset = MalwareDataset(X, y)\n",
    "    \n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    train_set, val_set = torch.utils.data.random_split(\n",
    "        dataset, [train_size, len(dataset)-train_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=128)\n",
    "\n",
    "    model = MalwareClassifier(X.shape[1]).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    best_acc = 0\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device).unsqueeze(1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val = X_val.to(device)\n",
    "                y_val = y_val.to(device).unsqueeze(1)\n",
    "                outputs = model(X_val)\n",
    "                predicted = (outputs >= 0.5).float()\n",
    "                correct += (predicted == y_val).sum().item()\n",
    "                total += y_val.size(0)\n",
    "        acc = correct / total\n",
    "        \n",
    "        print(f'Epoch {epoch+1:2d} | Loss: {total_loss/len(train_loader):.4f} | Acc: {acc:.4f}')\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save({\n",
    "                'model': model.state_dict(),\n",
    "                'mean': dataset.mean,\n",
    "                'std': dataset.std\n",
    "            }, model_save)\n",
    "    \n",
    "    print(f'Training complete. Best accuracy: {best_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2316 files using 8 processes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2316/2316 [01:57<00:00, 19.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved with 2316 samples\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    mp.set_start_method('fork')\n",
    "    prepare_dataset(num_processes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Loss: 0.6203 | Acc: 0.7500\n",
      "Epoch  2 | Loss: 0.4963 | Acc: 0.7802\n",
      "Epoch  3 | Loss: 0.4379 | Acc: 0.8147\n",
      "Epoch  4 | Loss: 0.3906 | Acc: 0.8168\n",
      "Epoch  5 | Loss: 0.3720 | Acc: 0.8362\n",
      "Epoch  6 | Loss: 0.3328 | Acc: 0.8556\n",
      "Epoch  7 | Loss: 0.3172 | Acc: 0.8427\n",
      "Epoch  8 | Loss: 0.3017 | Acc: 0.8664\n",
      "Epoch  9 | Loss: 0.2762 | Acc: 0.8621\n",
      "Epoch 10 | Loss: 0.2543 | Acc: 0.8685\n",
      "Epoch 11 | Loss: 0.2573 | Acc: 0.8793\n",
      "Epoch 12 | Loss: 0.2293 | Acc: 0.8707\n",
      "Epoch 13 | Loss: 0.2257 | Acc: 0.8836\n",
      "Epoch 14 | Loss: 0.2192 | Acc: 0.8879\n",
      "Epoch 15 | Loss: 0.2041 | Acc: 0.8836\n",
      "Epoch 16 | Loss: 0.1928 | Acc: 0.8815\n",
      "Epoch 17 | Loss: 0.1795 | Acc: 0.8944\n",
      "Epoch 18 | Loss: 0.1755 | Acc: 0.8966\n",
      "Epoch 19 | Loss: 0.1659 | Acc: 0.8772\n",
      "Epoch 20 | Loss: 0.1599 | Acc: 0.8987\n",
      "Epoch 21 | Loss: 0.1599 | Acc: 0.8966\n",
      "Epoch 22 | Loss: 0.1435 | Acc: 0.9030\n",
      "Epoch 23 | Loss: 0.1330 | Acc: 0.8944\n",
      "Epoch 24 | Loss: 0.1179 | Acc: 0.8987\n",
      "Epoch 25 | Loss: 0.1091 | Acc: 0.9009\n",
      "Epoch 26 | Loss: 0.1076 | Acc: 0.9095\n",
      "Epoch 27 | Loss: 0.1000 | Acc: 0.8944\n",
      "Epoch 28 | Loss: 0.1020 | Acc: 0.9052\n",
      "Epoch 29 | Loss: 0.0900 | Acc: 0.9073\n",
      "Epoch 30 | Loss: 0.0752 | Acc: 0.9030\n",
      "Epoch 31 | Loss: 0.0766 | Acc: 0.9009\n",
      "Epoch 32 | Loss: 0.0783 | Acc: 0.9009\n",
      "Epoch 33 | Loss: 0.0581 | Acc: 0.9159\n",
      "Epoch 34 | Loss: 0.0659 | Acc: 0.9009\n",
      "Epoch 35 | Loss: 0.0569 | Acc: 0.8987\n",
      "Epoch 36 | Loss: 0.0562 | Acc: 0.9095\n",
      "Epoch 37 | Loss: 0.0512 | Acc: 0.9030\n",
      "Epoch 38 | Loss: 0.0543 | Acc: 0.9095\n",
      "Epoch 39 | Loss: 0.0471 | Acc: 0.9159\n",
      "Epoch 40 | Loss: 0.0427 | Acc: 0.9073\n",
      "Epoch 41 | Loss: 0.0418 | Acc: 0.9073\n",
      "Epoch 42 | Loss: 0.0333 | Acc: 0.9095\n",
      "Epoch 43 | Loss: 0.0303 | Acc: 0.9203\n",
      "Epoch 44 | Loss: 0.0312 | Acc: 0.8987\n",
      "Epoch 45 | Loss: 0.0328 | Acc: 0.9116\n",
      "Epoch 46 | Loss: 0.0292 | Acc: 0.9073\n",
      "Epoch 47 | Loss: 0.0246 | Acc: 0.9203\n",
      "Epoch 48 | Loss: 0.0273 | Acc: 0.9138\n",
      "Epoch 49 | Loss: 0.0271 | Acc: 0.9052\n",
      "Epoch 50 | Loss: 0.0233 | Acc: 0.9095\n",
      "Training complete. Best accuracy: 0.9203\n"
     ]
    }
   ],
   "source": [
    "train_model('dataset/features.npy', 'dataset/labels.npy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
