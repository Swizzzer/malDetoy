{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Creating dataset...\n",
      "Loading benign samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Nozomi/Download/Malware/training/.venv/lib/python3.9/site-packages/pywt/_multilevel.py:43: UserWarning: Level value of 3 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading malware samples...\n",
      "Total samples loaded: 2076\n",
      "Feature dimension: 48\n",
      "Starting training...\n",
      "Epoch 1, Batch 0, Loss: 0.7036\n",
      "Epoch 1, Batch 10, Loss: 0.6122\n",
      "Epoch 1, Batch 20, Loss: 0.4915\n",
      "Epoch 1, Batch 30, Loss: 0.4913\n",
      "Epoch 1, Batch 40, Loss: 0.4673\n",
      "Epoch 1, Batch 50, Loss: 0.5219\n",
      "Epoch 1, Train Loss: 0.4922, Val Loss: 0.4165, Accuracy: 81.01%\n",
      "New best model saved with accuracy: 81.01%\n",
      "Epoch 2, Batch 0, Loss: 0.3420\n",
      "Epoch 2, Batch 10, Loss: 0.3357\n",
      "Epoch 2, Batch 20, Loss: 0.2833\n",
      "Epoch 2, Batch 30, Loss: 0.3188\n",
      "Epoch 2, Batch 40, Loss: 0.3993\n",
      "Epoch 2, Batch 50, Loss: 0.4736\n",
      "Epoch 2, Train Loss: 0.3728, Val Loss: 0.3678, Accuracy: 82.21%\n",
      "New best model saved with accuracy: 82.21%\n",
      "Epoch 3, Batch 0, Loss: 0.2380\n",
      "Epoch 3, Batch 10, Loss: 0.4743\n",
      "Epoch 3, Batch 20, Loss: 0.4253\n",
      "Epoch 3, Batch 30, Loss: 0.4791\n",
      "Epoch 3, Batch 40, Loss: 0.3208\n",
      "Epoch 3, Batch 50, Loss: 0.3171\n",
      "Epoch 3, Train Loss: 0.3491, Val Loss: 0.3558, Accuracy: 84.62%\n",
      "New best model saved with accuracy: 84.62%\n",
      "Epoch 4, Batch 0, Loss: 0.3383\n",
      "Epoch 4, Batch 10, Loss: 0.4220\n",
      "Epoch 4, Batch 20, Loss: 0.3747\n",
      "Epoch 4, Batch 30, Loss: 0.2554\n",
      "Epoch 4, Batch 40, Loss: 0.2416\n",
      "Epoch 4, Batch 50, Loss: 0.2313\n",
      "Epoch 4, Train Loss: 0.3417, Val Loss: 0.3296, Accuracy: 84.86%\n",
      "New best model saved with accuracy: 84.86%\n",
      "Epoch 5, Batch 0, Loss: 0.3836\n",
      "Epoch 5, Batch 10, Loss: 0.2224\n",
      "Epoch 5, Batch 20, Loss: 0.4252\n",
      "Epoch 5, Batch 30, Loss: 0.2694\n",
      "Epoch 5, Batch 40, Loss: 0.3518\n",
      "Epoch 5, Batch 50, Loss: 0.3894\n",
      "Epoch 5, Train Loss: 0.3286, Val Loss: 0.3389, Accuracy: 85.58%\n",
      "New best model saved with accuracy: 85.58%\n",
      "Epoch 6, Batch 0, Loss: 0.2777\n",
      "Epoch 6, Batch 10, Loss: 0.4201\n",
      "Epoch 6, Batch 20, Loss: 0.2011\n",
      "Epoch 6, Batch 30, Loss: 0.4666\n",
      "Epoch 6, Batch 40, Loss: 0.2503\n",
      "Epoch 6, Batch 50, Loss: 0.5252\n",
      "Epoch 6, Train Loss: 0.3294, Val Loss: 0.3571, Accuracy: 83.65%\n",
      "Epoch 7, Batch 0, Loss: 0.4019\n",
      "Epoch 7, Batch 10, Loss: 0.3711\n",
      "Epoch 7, Batch 20, Loss: 0.4664\n",
      "Epoch 7, Batch 30, Loss: 0.2395\n",
      "Epoch 7, Batch 40, Loss: 0.4246\n",
      "Epoch 7, Batch 50, Loss: 0.1918\n",
      "Epoch 7, Train Loss: 0.3088, Val Loss: 0.3618, Accuracy: 85.34%\n",
      "Epoch 8, Batch 0, Loss: 0.4025\n",
      "Epoch 8, Batch 10, Loss: 0.1890\n",
      "Epoch 8, Batch 20, Loss: 0.4244\n",
      "Epoch 8, Batch 30, Loss: 0.3086\n",
      "Epoch 8, Batch 40, Loss: 0.2422\n",
      "Epoch 8, Batch 50, Loss: 0.4248\n",
      "Epoch 8, Train Loss: 0.3096, Val Loss: 0.3310, Accuracy: 85.58%\n",
      "Epoch 9, Batch 0, Loss: 0.4188\n",
      "Epoch 9, Batch 10, Loss: 0.3990\n",
      "Epoch 9, Batch 20, Loss: 0.2905\n",
      "Epoch 9, Batch 30, Loss: 0.3763\n",
      "Epoch 9, Batch 40, Loss: 0.1425\n",
      "Epoch 9, Batch 50, Loss: 0.2235\n",
      "Epoch 9, Train Loss: 0.3372, Val Loss: 0.3321, Accuracy: 86.30%\n",
      "New best model saved with accuracy: 86.30%\n",
      "Epoch 10, Batch 0, Loss: 0.3226\n",
      "Epoch 10, Batch 10, Loss: 0.5161\n",
      "Epoch 10, Batch 20, Loss: 0.3683\n",
      "Epoch 10, Batch 30, Loss: 0.4044\n",
      "Epoch 10, Batch 40, Loss: 0.2635\n",
      "Epoch 10, Batch 50, Loss: 0.3505\n",
      "Epoch 10, Train Loss: 0.2906, Val Loss: 0.3425, Accuracy: 85.58%\n",
      "Epoch 11, Batch 0, Loss: 0.2095\n",
      "Epoch 11, Batch 10, Loss: 0.4604\n",
      "Epoch 11, Batch 20, Loss: 0.4384\n",
      "Epoch 11, Batch 30, Loss: 0.1987\n",
      "Epoch 11, Batch 40, Loss: 0.2442\n",
      "Epoch 11, Batch 50, Loss: 0.2918\n",
      "Epoch 11, Train Loss: 0.2865, Val Loss: 0.3031, Accuracy: 86.78%\n",
      "New best model saved with accuracy: 86.78%\n",
      "Epoch 12, Batch 0, Loss: 0.2829\n",
      "Epoch 12, Batch 10, Loss: 0.3562\n",
      "Epoch 12, Batch 20, Loss: 0.2814\n",
      "Epoch 12, Batch 30, Loss: 0.2342\n",
      "Epoch 12, Batch 40, Loss: 0.3728\n",
      "Epoch 12, Batch 50, Loss: 0.1797\n",
      "Epoch 12, Train Loss: 0.3092, Val Loss: 0.3148, Accuracy: 85.34%\n",
      "Epoch 13, Batch 0, Loss: 0.3689\n",
      "Epoch 13, Batch 10, Loss: 0.3716\n",
      "Epoch 13, Batch 20, Loss: 0.1535\n",
      "Epoch 13, Batch 30, Loss: 0.2988\n",
      "Epoch 13, Batch 40, Loss: 0.2307\n",
      "Epoch 13, Batch 50, Loss: 0.1908\n",
      "Epoch 13, Train Loss: 0.3068, Val Loss: 0.3357, Accuracy: 84.86%\n",
      "Epoch 14, Batch 0, Loss: 0.2543\n",
      "Epoch 14, Batch 10, Loss: 0.3351\n",
      "Epoch 14, Batch 20, Loss: 0.4014\n",
      "Epoch 14, Batch 30, Loss: 0.2844\n",
      "Epoch 14, Batch 40, Loss: 0.4180\n",
      "Epoch 14, Batch 50, Loss: 0.2729\n",
      "Epoch 14, Train Loss: 0.2892, Val Loss: 0.3204, Accuracy: 87.02%\n",
      "New best model saved with accuracy: 87.02%\n",
      "Epoch 15, Batch 0, Loss: 0.3909\n",
      "Epoch 15, Batch 10, Loss: 0.3064\n",
      "Epoch 15, Batch 20, Loss: 0.4886\n",
      "Epoch 15, Batch 30, Loss: 0.5794\n",
      "Epoch 15, Batch 40, Loss: 0.5971\n",
      "Epoch 15, Batch 50, Loss: 0.2820\n",
      "Epoch 15, Train Loss: 0.2831, Val Loss: 0.3488, Accuracy: 86.30%\n",
      "Epoch 16, Batch 0, Loss: 0.1917\n",
      "Epoch 16, Batch 10, Loss: 0.2171\n",
      "Epoch 16, Batch 20, Loss: 0.3102\n",
      "Epoch 16, Batch 30, Loss: 0.1887\n",
      "Epoch 16, Batch 40, Loss: 0.2321\n",
      "Epoch 16, Batch 50, Loss: 0.3477\n",
      "Epoch 16, Train Loss: 0.2758, Val Loss: 0.3228, Accuracy: 87.02%\n",
      "Epoch 17, Batch 0, Loss: 0.2655\n",
      "Epoch 17, Batch 10, Loss: 0.2981\n",
      "Epoch 17, Batch 20, Loss: 0.3318\n",
      "Epoch 17, Batch 30, Loss: 0.2888\n",
      "Epoch 17, Batch 40, Loss: 0.2857\n",
      "Epoch 17, Batch 50, Loss: 0.5514\n",
      "Epoch 17, Train Loss: 0.3011, Val Loss: 0.3400, Accuracy: 86.06%\n",
      "Epoch 18, Batch 0, Loss: 0.4256\n",
      "Epoch 18, Batch 10, Loss: 0.2662\n",
      "Epoch 18, Batch 20, Loss: 0.3364\n",
      "Epoch 18, Batch 30, Loss: 0.3184\n",
      "Epoch 18, Batch 40, Loss: 0.2491\n",
      "Epoch 18, Batch 50, Loss: 0.2530\n",
      "Epoch 18, Train Loss: 0.2630, Val Loss: 0.3417, Accuracy: 86.54%\n",
      "Epoch 19, Batch 0, Loss: 0.4268\n",
      "Epoch 19, Batch 10, Loss: 0.2359\n",
      "Epoch 19, Batch 20, Loss: 0.2295\n",
      "Epoch 19, Batch 30, Loss: 0.3554\n",
      "Epoch 19, Batch 40, Loss: 0.2410\n",
      "Epoch 19, Batch 50, Loss: 0.3986\n",
      "Epoch 19, Train Loss: 0.2735, Val Loss: 0.3055, Accuracy: 86.78%\n",
      "Epoch 20, Batch 0, Loss: 0.2239\n",
      "Epoch 20, Batch 10, Loss: 0.2293\n",
      "Epoch 20, Batch 20, Loss: 0.3606\n",
      "Epoch 20, Batch 30, Loss: 0.2536\n",
      "Epoch 20, Batch 40, Loss: 0.1338\n",
      "Epoch 20, Batch 50, Loss: 0.1838\n",
      "Epoch 20, Train Loss: 0.2621, Val Loss: 0.3263, Accuracy: 85.82%\n",
      "Training completed. Best accuracy: 87.02%\n",
      "Model saved in directory: models/malware_detector_20250215_201934\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pefile\n",
    "import math\n",
    "from collections import Counter\n",
    "import pywt\n",
    "from scipy import stats\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# 设置随机种子以确保可重复性\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class EnhancedFeatureExtractor:\n",
    "    def __init__(self, wavelet='haar', level=3):\n",
    "        self.wavelet = wavelet\n",
    "        self.level = level\n",
    "        # 预计算特征维度\n",
    "        self.expected_feature_dim = self._calculate_expected_dim()\n",
    "        \n",
    "    def _calculate_expected_dim(self):\n",
    "        \"\"\"计算预期的特征维度\"\"\"\n",
    "        # PE特征维度 (固定)\n",
    "        pe_features = 8  # 文件熵(1) + 节特征(3) + 导出表特征(4)\n",
    "        \n",
    "        # 小波特征维度 (固定)\n",
    "        # 对于每个分解级别，我们有3个细节系数矩阵(水平、垂直、对角线)\n",
    "        # 每个矩阵提供4个统计量(均值、标准差、偏度、峰度)\n",
    "        wavelet_features = 4  # 近似系数的4个统计量\n",
    "        wavelet_features += 3 * 4 * self.level  # 细节系数的统计量\n",
    "        \n",
    "        return pe_features + wavelet_features\n",
    "    def extract_export_features(self, pe):\n",
    "        \"\"\"提取导出表特征\"\"\"\n",
    "        features = []\n",
    "        try:\n",
    "            if hasattr(pe, 'DIRECTORY_ENTRY_EXPORT'):\n",
    "                exports = pe.DIRECTORY_ENTRY_EXPORT.symbols\n",
    "                num_exports = len(exports) if exports else 0\n",
    "                features.extend([\n",
    "                    num_exports,\n",
    "                    len(pe.DIRECTORY_ENTRY_EXPORT.name) if hasattr(pe.DIRECTORY_ENTRY_EXPORT, 'name') else 0,\n",
    "                    sum(1 for e in exports if e.name) if exports else 0,\n",
    "                    pe.OPTIONAL_HEADER.DATA_DIRECTORY[pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_EXPORT']].Size\n",
    "                ])\n",
    "            else:\n",
    "                features.extend([0] * 4)\n",
    "        except:\n",
    "            features.extend([0] * 4)\n",
    "    \n",
    "        # 确保返回4个特征\n",
    "        return features[:4] if len(features) >=4 else features + [0]*(4-len(features))\n",
    "    def calculate_entropy(self, data):\n",
    "        \"\"\"计算数据的熵值\"\"\"\n",
    "        if not data:\n",
    "            return 0\n",
    "        \n",
    "        # 使用Counter来计算字节频率\n",
    "        occurrences = Counter(data)\n",
    "        total_bytes = len(data)\n",
    "        entropy = 0\n",
    "        \n",
    "        # 计算香农熵\n",
    "        for count in occurrences.values():\n",
    "            probability = count / total_bytes\n",
    "            entropy -= probability * math.log2(probability)\n",
    "            \n",
    "        return entropy\n",
    "    def extract_section_features(self, pe):\n",
    "        \"\"\"提取固定数量的节特征\"\"\"\n",
    "        features = []\n",
    "        try:\n",
    "            if hasattr(pe, 'sections') and len(pe.sections) > 0:\n",
    "                section = pe.sections[0]  # 只使用第一个节\n",
    "                section_data = section.get_data()\n",
    "                features.extend([\n",
    "                    len(section_data),\n",
    "                    self.calculate_entropy(section_data),\n",
    "                    section.Characteristics,\n",
    "                ])\n",
    "            else:\n",
    "                features.extend([0] * 3)\n",
    "        except:\n",
    "            features.extend([0] * 3)\n",
    "        return features\n",
    "\n",
    "    def extract_wavelet_features(self, image_array):\n",
    "        \"\"\"提取固定维度的小波特征\"\"\"\n",
    "        try:\n",
    "            coeffs = pywt.wavedec2(image_array, self.wavelet, level=self.level)\n",
    "            features = []\n",
    "            \n",
    "            # 处理近似系数\n",
    "            features.extend([\n",
    "                np.mean(coeffs[0]),\n",
    "                np.std(coeffs[0]),\n",
    "                stats.skew(coeffs[0].ravel()),\n",
    "                stats.kurtosis(coeffs[0].ravel())\n",
    "            ])\n",
    "            \n",
    "            # 处理细节系数\n",
    "            for detail_coeffs in coeffs[1:]:\n",
    "                for detail in detail_coeffs:\n",
    "                    features.extend([\n",
    "                        np.mean(detail),\n",
    "                        np.std(detail),\n",
    "                        stats.skew(detail.ravel()),\n",
    "                        stats.kurtosis(detail.ravel())\n",
    "                    ])\n",
    "            \n",
    "            # 确保特征维度正确\n",
    "            expected_wavelet_features = 4 + (3 * 4 * self.level)\n",
    "            if len(features) < expected_wavelet_features:\n",
    "                features.extend([0] * (expected_wavelet_features - len(features)))\n",
    "            elif len(features) > expected_wavelet_features:\n",
    "                features = features[:expected_wavelet_features]\n",
    "                \n",
    "            return features\n",
    "        except Exception as e:\n",
    "            print(f\"Error in wavelet feature extraction: {str(e)}\")\n",
    "            return [0] * (4 + (3 * 4 * self.level))\n",
    "\n",
    "    def extract_features(self, file_path):\n",
    "        \"\"\"提取固定维度的特征集\"\"\"\n",
    "        try:\n",
    "            features = []\n",
    "            \n",
    "            # 读取文件\n",
    "            with open(file_path, 'rb') as f:\n",
    "                data = f.read()\n",
    "            \n",
    "            # 1. 文件熵\n",
    "            file_entropy = self.calculate_entropy(data)\n",
    "            features.append(file_entropy)\n",
    "            \n",
    "            # 2. PE特征\n",
    "            try:\n",
    "                pe = pefile.PE(file_path)\n",
    "                features.extend(self.extract_section_features(pe))\n",
    "                features.extend(self.extract_export_features(pe))\n",
    "            except:\n",
    "                features.extend([0] * 7)  # PE特征的默认值\n",
    "            \n",
    "            # 3. 小波特征\n",
    "            image_array = np.frombuffer(data, dtype=np.uint8)\n",
    "            width = 384\n",
    "            height = len(image_array) // width + (1 if len(image_array) % width else 0)\n",
    "            padded_size = height * width\n",
    "            \n",
    "            if len(image_array) < padded_size:\n",
    "                image_array = np.pad(image_array, (0, padded_size - len(image_array)))\n",
    "            \n",
    "            image_array = image_array.reshape((height, width))\n",
    "            wavelet_features = self.extract_wavelet_features(image_array)\n",
    "            features.extend(wavelet_features)\n",
    "            \n",
    "            # 确保特征维度正确\n",
    "            if len(features) != self.expected_feature_dim:\n",
    "                print(f\"Warning: Feature dimension mismatch for {file_path}\")\n",
    "                if len(features) < self.expected_feature_dim:\n",
    "                    features.extend([0] * (self.expected_feature_dim - len(features)))\n",
    "                else:\n",
    "                    features = features[:self.expected_feature_dim]\n",
    "            \n",
    "            return np.array(features, dtype=np.float32)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting features from {file_path}: {str(e)}\")\n",
    "            return np.zeros(self.expected_feature_dim, dtype=np.float32)\n",
    "\n",
    "\n",
    "# 数据集类定义\n",
    "class EnhancedMalwareDataset(Dataset):\n",
    "    def __init__(self, benign_dir, malware_dir):\n",
    "        self.data = []\n",
    "        self.feature_extractor = EnhancedFeatureExtractor()\n",
    "        \n",
    "        print(\"Loading benign samples...\")\n",
    "        for filename in os.listdir(benign_dir):\n",
    "            file_path = os.path.join(benign_dir, filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                features = self.feature_extractor.extract_features(file_path)\n",
    "                self.data.append((features, 0))\n",
    "        \n",
    "        print(\"Loading malware samples...\")\n",
    "        for filename in os.listdir(malware_dir):\n",
    "            file_path = os.path.join(malware_dir, filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                features = self.feature_extractor.extract_features(file_path)\n",
    "                self.data.append((features, 1))\n",
    "        \n",
    "        print(f\"Total samples loaded: {len(self.data)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features, label = self.data[idx]\n",
    "        return torch.FloatTensor(features), label\n",
    "\n",
    "# 模型类定义\n",
    "class EnhancedMalwareDetector(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(EnhancedMalwareDetector, self).__init__()\n",
    "        \n",
    "        self.pe_features = nn.Sequential(\n",
    "            nn.Linear(8, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        wavelet_feature_size = input_size - 8\n",
    "        self.wavelet_features = nn.Sequential(\n",
    "            nn.Linear(wavelet_feature_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64)\n",
    "        )\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(96, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(16, 2)\n",
    "        )\n",
    "        \n",
    "        self.attention_pe = nn.Sequential(\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.attention_wavelet = nn.Sequential(\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        pe_x = x[:, :8]\n",
    "        wavelet_x = x[:, 8:]\n",
    "        \n",
    "        pe_features = self.pe_features(pe_x)\n",
    "        pe_attention = self.attention_pe(pe_features)\n",
    "        pe_features = pe_features * pe_attention\n",
    "        \n",
    "        wavelet_features = self.wavelet_features(wavelet_x)\n",
    "        wavelet_attention = self.attention_wavelet(wavelet_features)\n",
    "        wavelet_features = wavelet_features * wavelet_attention\n",
    "        \n",
    "        combined_features = torch.cat((pe_features, wavelet_features), dim=1)\n",
    "        fused_features = self.fusion(combined_features)\n",
    "        \n",
    "        output = self.classifier(fused_features)\n",
    "        return output\n",
    "\n",
    "# 训练主函数\n",
    "def train_model(benign_dir, malware_dir, model_dir, epochs=20, batch_size=32):\n",
    "    # 创建模型保存目录\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # 设置设备\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # 创建数据集\n",
    "    print(\"Creating dataset...\")\n",
    "    dataset = EnhancedMalwareDataset(benign_dir, malware_dir)\n",
    "    \n",
    "    \n",
    "    # 获取特征维度\n",
    "    input_size = dataset[0][0].shape[0]\n",
    "    print(f\"Feature dimension: {input_size}\")\n",
    "    \n",
    "    # 分割数据集\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, test_size]\n",
    "    )\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, drop_last=True)\n",
    "    \n",
    "    # 创建模型\n",
    "    model = EnhancedMalwareDetector(input_size).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', patience=3, factor=0.5\n",
    "    )\n",
    "    \n",
    "    # 训练记录\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    accuracies = []\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    # 训练循环\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, (features, labels) in enumerate(train_loader):\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}, Batch {i}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(epoch_loss)\n",
    "        \n",
    "        # 验证\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for features, labels in test_loader:\n",
    "                features, labels = features.to(device), labels.to(device)\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss = val_loss / len(test_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}, Train Loss: {epoch_loss:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "        \n",
    "        # 更新学习率\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            model_path = os.path.join(model_dir, f'best_model_{epoch+1}.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'accuracy': accuracy,\n",
    "                'input_size': input_size\n",
    "            }, model_path)\n",
    "            print(f\"New best model saved with accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    # 绘制训练过程\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(accuracies, label='Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(model_dir, 'training_plot.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    return input_size, best_accuracy\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 设置路径\n",
    "    benign_dir = \"white_files\"\n",
    "    malware_dir = \"black_files\"\n",
    "    model_dir = f\"models/malware_detector_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "    # 训练模型\n",
    "    input_size, best_accuracy = train_model(\n",
    "        benign_dir=benign_dir,\n",
    "        malware_dir=malware_dir,\n",
    "        model_dir=model_dir,\n",
    "        epochs=20,\n",
    "        batch_size=32\n",
    "    )\n",
    "    \n",
    "    print(f\"Training completed. Best accuracy: {best_accuracy:.2f}%\")\n",
    "    print(f\"Model saved in directory: {model_dir}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
