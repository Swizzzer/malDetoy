{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pefile\n",
    "import pywt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "\n",
    "def calculate_entropy(data):\n",
    "    if not data:\n",
    "        return 0.0\n",
    "    counts = Counter(data)\n",
    "    entropy = 0.0\n",
    "    total = len(data)\n",
    "    for count in counts.values():\n",
    "        p = count / total\n",
    "        entropy -= p * math.log2(p)\n",
    "    return entropy\n",
    "\n",
    "def get_import_features(pe, max_dim=128):\n",
    "    features = [0] * max_dim\n",
    "    try:\n",
    "        if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT'):\n",
    "            imports = []\n",
    "            for entry in pe.DIRECTORY_ENTRY_IMPORT:\n",
    "                for imp in entry.imports:\n",
    "                    if imp.name:\n",
    "                        imports.append(imp.name.decode('utf-8', 'ignore'))\n",
    "            for i, func in enumerate(imports[:max_dim]):\n",
    "                features[i] = hash(func) % 1000  # Hash simplification\n",
    "    except:\n",
    "        pass\n",
    "    return features\n",
    "\n",
    "def get_section_features(pe, max_sections=8):\n",
    "    sections = []\n",
    "    try:\n",
    "        for section in pe.sections[:max_sections]:\n",
    "            sections.extend([\n",
    "                section.get_entropy(),\n",
    "                section.SizeOfRawData,\n",
    "                section.Characteristics\n",
    "            ])\n",
    "    except:\n",
    "        pass\n",
    "    pad_length = max_sections*3 - len(sections)\n",
    "    return sections + [0]*pad_length\n",
    "\n",
    "def get_image_features(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = f.read()\n",
    "        \n",
    "        arr = np.frombuffer(data, dtype=np.uint8)\n",
    "        \n",
    "        actual_size = int(np.ceil(np.sqrt(len(arr))))\n",
    "        actual_size = actual_size - (actual_size % 2)  # Ensure even size for wavelet\n",
    "        \n",
    "        # Reshape\n",
    "        arr = arr[:actual_size*actual_size]\n",
    "        arr = np.pad(arr, (0, actual_size*actual_size - len(arr)))\n",
    "        img = arr.reshape(actual_size, actual_size).astype(float)\n",
    "        \n",
    "        img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
    "        \n",
    "        coeffs = pywt.wavedec2(img, 'haar', level=3)\n",
    "        features = []\n",
    "        \n",
    "        for coef in coeffs:\n",
    "        \n",
    "            if isinstance(coef, tuple):\n",
    "                for detail_coef in coef:\n",
    "                    features.extend([\n",
    "                        np.mean(np.abs(detail_coef)),  # Mean energy\n",
    "                        np.std(detail_coef),           # Standard deviation\n",
    "                        np.percentile(detail_coef, 90),# 90th percentile\n",
    "                        np.sum(detail_coef < 0),       # Number of negative coefficients\n",
    "                        entropy_measure(detail_coef)    # Entropy of coefficients\n",
    "                    ])\n",
    "            else:\n",
    "                features.extend([\n",
    "                    np.mean(np.abs(coef)),            # Mean energy\n",
    "                    np.std(coef),                     # Standard deviation\n",
    "                    np.percentile(coef, 90),          # 90th percentile\n",
    "                    np.sum(coef < 0),                 # Number of negative coefficients\n",
    "                    entropy_measure(coef)             # Entropy of coefficients\n",
    "                ])\n",
    "        for coef in coeffs:\n",
    "            flattened = np.array(coef).flatten()\n",
    "            features.extend(flattened.tolist())\n",
    "            \n",
    "        expected_length = 1024\n",
    "        if len(features) < expected_length:\n",
    "            features.extend([0.0] * (expected_length - len(features)))\n",
    "        \n",
    "        return features[:expected_length]\n",
    "    \n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        return [0.0] * 1024\n",
    "\n",
    "def entropy_measure(coeffs):\n",
    "    coeffs = np.abs(coeffs)\n",
    "    coeffs = coeffs / (np.sum(coeffs) + 1e-8)\n",
    "    entropy = -np.sum(coeffs * np.log2(coeffs + 1e-8))\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def extract_file_features(file_path):\n",
    "    try:\n",
    "        pe = pefile.PE(file_path)\n",
    "    except:\n",
    "        pe = None\n",
    "    \n",
    "    features = []\n",
    "    # Entropy\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = f.read()\n",
    "    features.append(calculate_entropy(data))\n",
    "    \n",
    "    # Import features\n",
    "    features += get_import_features(pe) if pe else [0]*128\n",
    "    \n",
    "    # Section features\n",
    "    features += get_section_features(pe) if pe else [0]*24\n",
    "    \n",
    "    # Wavelet features\n",
    "    features += get_image_features(file_path)\n",
    "    \n",
    "    return np.array(features, dtype=np.float32)\n",
    "\n",
    "def process_file(args):\n",
    "    file_path, label = args\n",
    "    features = extract_file_features(file_path)\n",
    "    return features, label\n",
    "\n",
    "class MalwareClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def scan(model_path='model.pt', scan_dir='scan_files'):\n",
    "    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model = MalwareClassifier(len(checkpoint['mean'])).to(device)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    model.eval()\n",
    "    \n",
    "    # move to CPU\n",
    "    mean_np = checkpoint['mean'].cpu().numpy()\n",
    "    std_np = checkpoint['std'].cpu().numpy()\n",
    "    \n",
    "    total = 0\n",
    "    detected = 0\n",
    "    for root, _, files in os.walk(scan_dir):\n",
    "        for f in files:\n",
    "            file_path = os.path.join(root, f)\n",
    "            features = extract_file_features(file_path)\n",
    "            \n",
    "            features = (features - mean_np) / std_np\n",
    "            features_tensor = torch.FloatTensor(features).unsqueeze(0).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                prob = model(features_tensor).item()\n",
    "            \n",
    "            result = 'Malicious' if prob >= 0.75 else 'Benign'\n",
    "            print(f'{f}: {result} ({prob:.4f})')\n",
    "            \n",
    "            total += 1\n",
    "            if prob >= 0.5:\n",
    "                detected += 1\n",
    "    \n",
    "    if total > 0:\n",
    "        print(f'\\nDetection Rate: {detected/total:.2%} ({detected}/{total})')\n",
    "    else:\n",
    "        print('No files found.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store: Benign (0.0539)\n",
      "1l00o86h57u2.sys: Benign (0.0122)\n",
      "0ktWo9DdkxVW.sys: Benign (0.0166)\n",
      "\n",
      "Detection Rate: 0.00% (0/3)\n"
     ]
    }
   ],
   "source": [
    "scan()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
